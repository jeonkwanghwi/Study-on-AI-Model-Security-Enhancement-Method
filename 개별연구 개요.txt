AI모델 보안 강화 방법 연구

최근 AI 모델을 해킹 및 악용하기 위한 다양한 사례가 발생하고 있음. 본 연구는 안전하게 AI 모델을 사용할 수 있도록, AI 모델 생성을 위한 데이터 수집, 학습, 평가, 활용시에 발생할 수 있는 보안 취약점을 분석하고 이를 사전에 제거하거나 방어, 검출하기 위한 기법을 연구한다.


프로그래밍언어 : 파이썬

https://github.com/Trusted-AI/adversarial-robustness-toolbox


===============================================
교수님과의 미팅 메모 (조교 정리)

	AI모델 보안 강화 방법 연구
-	좋은 프로그램이라는 개념. 소프트웨어는 특정한 문제를 얼마나 효율적으로 해결할 수 있느냐를 먼저 만족해야 함-> 과거에는 컴퓨팅 파워나 용량이 한정적이었기 때문

-	알고리즘 학문에서는 이 문제를 효율적으로 풀수 있는 가장 베스트 답이 무엇인가. 정렬, 검색 문제에 가장 하한까지 최적화할 수 있는 방법이 증명이 되면 그것보다 효율적인 방법을 찾을 필요 없음

-	기존에는 효율성만 생각하다가 공학적인 기법을 도입하게됨. 그게 소프트웨어 엔지니어링, 공학의 목표: 기존의 문제나 기존의 프로세스를 더 효율적으로 만들기 위한 고민이 공학 -> 시스템이 계속해서 해킹 당하는 문제가 발생

-	ai상황이 소프트웨어 초기와 비슷, 분류, 예측만 잘하면 됨. 알고리즘단계와 같은 상황. 재활용성, 지속가능성, 보안, 안정성이 배제되어 있음

-	ai 모델들은 특히 적대적 공격에 아주 취약함

-	APRICOT 데이터셋 참고(https://paperswithcode.com/paper/apricot-a-dataset-of-physical-adversarial)

-	적대적 공격을 피지컬하게 하는 것, 교통 표지판에 스티커 하나만 붙여도 오인식됨
어떻게 이런 부분을 잘 막을 수 있을까?
